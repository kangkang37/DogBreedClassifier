{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0deb6a90",
   "metadata": {},
   "source": [
    "# Dog Breed Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "060b9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# dataset\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# save result\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b3339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device : mps\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2022) # Set the random seed so that the random numbers generated are the same each time\n",
    "try:\n",
    "    device = torch.device(\"mps\") \n",
    "except:\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f'Current Device : {device}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75cdcea",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1394328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in Dataset : 10222\n"
     ]
    }
   ],
   "source": [
    "img_names = glob.glob(\"./dog-breed-identification/train/*.jpg\") \n",
    "\n",
    "print(f'Total images in Dataset : {len(img_names)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e497678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dog-breed-identification/train/84accc2dc9f5bb3ebee89fe1bf23639c.jpg\n",
      "<class 'str'>\n",
      "(500, 430)\n",
      "(400, 300)\n",
      "(272, 350)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(img_names[0])\n",
    "print(type(img_names[0]))\n",
    "\n",
    "img = Image.open(img_names[0])\n",
    "print(img.size)\n",
    "\n",
    "img1 = Image.open(img_names[1])\n",
    "print(img1.size)\n",
    "\n",
    "img2 = Image.open(img_names[2])\n",
    "print(img2.size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea6b07a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "471214ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a class called DogDataset, which is a custom PyTorch dataset class\n",
    "\n",
    "class DogDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_path, csv_path):\n",
    "        self.csv_path = csv_path\n",
    "        self.transform = None\n",
    "\n",
    "        self.img_names = glob.glob(f\"{img_path}/*.jpg\") \n",
    "\n",
    "        if csv_path:\n",
    "            label_df = pd.read_csv(csv_path)\n",
    "            self.label_idx2name = label_df['breed'].unique() \n",
    "            \n",
    "            self.label_name2idx = {} \n",
    "            for i in range(len(self.label_idx2name)):\n",
    "                self.label_name2idx[self.label_idx2name[i]] = i\n",
    "                \n",
    "            \n",
    "            self.img2label = {}\n",
    "            for _, row in label_df.iterrows():\n",
    "                self.img2label[f\"{img_path}/{row['id']}.jpg\"] = self.label_name2idx[row['breed']]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        img = self.img_names[index]\n",
    "\n",
    "        if self.csv_path:\n",
    "            label = self.img2label[img]\n",
    "            label = torch.tensor(label)\n",
    "        else:\n",
    "            label = -1\n",
    "        \n",
    "        img = Image.open(img).convert(\"RGB\")\n",
    "        \n",
    "        img = self.transform(img)\n",
    "        \n",
    "        return (img, label) \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455c93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8413f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two preprocessing functions\n",
    "# vit_train_transform_fn -- used for preprocessing the train dataset\n",
    "# vit_valid_transform_fn -- used for preprocessing the valid dataset\n",
    "\n",
    "# The goal is to adjust the original images to the format suitable for the pre-trained Vision Transformer (ViT) model.\n",
    "# The pre-trained ViT model: the input images are 224x224 RGB images, and the pixel values of the images are normalized (i.e., mean subtraction and standard deviation division).\n",
    "\n",
    "channel_mean = torch.Tensor([0.485, 0.456, 0.406])\n",
    "channel_std = torch.Tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "vit_train_transform_fn = transforms.Compose([\n",
    "    transforms.Resize(256), \n",
    "    transforms.CenterCrop(224), \n",
    "\n",
    "    transforms.RandomHorizontalFlip(p=0.6),\n",
    "    transforms.RandomRotation(degrees=(30)),\n",
    "\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=channel_mean, std=channel_std), \n",
    "])\n",
    "\n",
    "vit_valid_transform_fn = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=channel_mean, std=channel_std),\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdfe736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = DogDataset(\n",
    "    img_path=\"./dog-breed-identification/train\",\n",
    "    csv_path=\"./dog-breed-identification/labels.csv\",\n",
    ")\n",
    "\n",
    "# print(type(dataset))\n",
    "# print(dataset[0])\n",
    "# csv_path=\"./dog-breed-identification/labels.csv\",\n",
    "# label_df = pd.read_csv(csv_path) # label_df是pandas的DataFrame\n",
    "# label_idx2name = label_df['breed'].unique()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b5e5ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train_dataset: 9199\n",
      "Number of samples in valid_dataset: 1023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "indexes = list(range(len(dataset)))\n",
    "\n",
    "train_indexes, valid_indexes = train_test_split(indexes, test_size=0.1)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indexes) \n",
    "valid_dataset = Subset(dataset, valid_indexes)\n",
    "\n",
    "print(f\"Number of samples in train_dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in valid_dataset: {len(valid_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02127c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.transform = vit_train_transform_fn\n",
    "train_dataset.transform = vit_train_transform_fn\n",
    "valid_dataset.transform = vit_valid_transform_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e616bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_valid_dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8, \n",
    "    shuffle=True\n",
    "    \n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46060e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The main purpose of this function is to visualize a batch of image samples,\n",
    "# helping us to intuitively understand the content and labels of these samples.\n",
    "\n",
    "def show_samples(batch_img, batch_label=None, num_samples=16):\n",
    "    sample_idx = 0\n",
    "    total_col = 4\n",
    "    total_row = math.ceil(num_samples / 4)\n",
    "    col_idx = 0\n",
    "    row_idx = 0\n",
    "\n",
    "    fig, axs = plt.subplots(total_row, total_col, figsize=(15, 15))\n",
    "    \n",
    "    while sample_idx < num_samples:\n",
    "        img = batch_img[sample_idx] \n",
    "        img = img.view(3, -1) * channel_std.view(3, -1) + channel_mean.view(3, -1)\n",
    "        \n",
    "        img = img.view(3, 224, 224) \n",
    "        \n",
    "        img = img.permute(1, 2, 0)\n",
    "        \n",
    "        axs[row_idx, col_idx].imshow(img) \n",
    "\n",
    "        if batch_label != None: \n",
    "            axs[row_idx, col_idx].set_title(dataset.label_idx2name[(batch_label[sample_idx])])\n",
    "\n",
    "        sample_idx += 1\n",
    "        col_idx += 1\n",
    "        if col_idx == 4: \n",
    "            col_idx = 0\n",
    "            row_idx += 1\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_img, batch_label = next(iter(train_dataloader))\n",
    "\n",
    "show_samples(batch_img, batch_label, 8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea52b52",
   "metadata": {},
   "source": [
    "## build model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a09948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainViT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PretrainViT, self).__init__()\n",
    "\n",
    "        \n",
    "        model = models.vit_l_16() ###\n",
    "        model.load_state_dict(torch.load('./vit_l_16-852ce7e3.pth')) ###\n",
    "\n",
    "\n",
    "        num_classifier_feature = model.heads.head.in_features\n",
    "        \n",
    "        \n",
    "        model.heads.head = nn.Sequential(\n",
    "            nn.Linear(num_classifier_feature, 120)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "        for param in self.model.named_parameters():\n",
    "            if \"heads\" not in param[0]:\n",
    "                param[1].requires_grad = False\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4dfc4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of paramaters: 123000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net = PretrainViT() \n",
    "net.to(device) \n",
    "print(f\"number of paramaters: {sum([param.numel() for param in net.parameters() if param.requires_grad])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219202b8",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "322b2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.009, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f896bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_accuracy(output, label):\n",
    "    output = output.to(\"cpu\") \n",
    "    label = label.to(\"cpu\")\n",
    "\n",
    "    sm = F.softmax(output, dim=1)\n",
    "    _, index = torch.max(sm, dim=1)\n",
    "    \n",
    "    return torch.sum((label == index)) / label.size()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9796d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0 \n",
    "    total_loss = 0.0 \n",
    "    running_acc = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    for batch_idx, (batch_img, batch_label) in enumerate(dataloader): \n",
    "\n",
    "        batch_img = batch_img.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        output = net(batch_img) \n",
    "        \n",
    "        loss = criterion(output, batch_label) \n",
    "        \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        acc = get_accuracy(output, batch_label) \n",
    "        \n",
    "        running_acc += acc\n",
    "        total_acc += acc\n",
    "        \n",
    "        if batch_idx % 100 == 0 and batch_idx != 0:\n",
    "            print(f\"[step: {batch_idx:4d}/{len(dataloader)}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "    \n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "250d7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    for batch_idx, (batch_img, batch_label) in enumerate(dataloader):\n",
    "\n",
    "        batch_img = batch_img.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        output = net(batch_img)\n",
    "        loss = criterion(output, batch_label)\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        acc = get_accuracy(output, batch_label)\n",
    "        total_acc += acc\n",
    "    \n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35d548a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  100/1150] loss: 2.540\n",
      "[step:  200/1150] loss: 0.491\n",
      "[step:  300/1150] loss: 0.224\n",
      "[step:  400/1150] loss: 0.210\n",
      "[step:  500/1150] loss: 0.179\n",
      "[step:  600/1150] loss: 0.166\n",
      "[step:  700/1150] loss: 0.146\n",
      "[step:  800/1150] loss: 0.188\n",
      "[step:  900/1150] loss: 0.148\n",
      "[step: 1000/1150] loss: 0.142\n",
      "[step: 1100/1150] loss: 0.129\n",
      "Epoch:  0, training loss: 0.400, training acc: 0.924 validation loss: 0.146, validation acc: 0.963\n",
      "[step:  100/1150] loss: 0.098\n",
      "[step:  200/1150] loss: 0.091\n",
      "[step:  300/1150] loss: 0.073\n",
      "[step:  400/1150] loss: 0.090\n",
      "[step:  500/1150] loss: 0.083\n",
      "[step:  600/1150] loss: 0.069\n",
      "[step:  700/1150] loss: 0.106\n",
      "[step:  800/1150] loss: 0.083\n",
      "[step:  900/1150] loss: 0.078\n",
      "[step: 1000/1150] loss: 0.102\n",
      "[step: 1100/1150] loss: 0.077\n",
      "Epoch:  1, training loss: 0.086, training acc: 0.975 validation loss: 0.137, validation acc: 0.961\n",
      "[step:  100/1150] loss: 0.059\n",
      "[step:  200/1150] loss: 0.064\n",
      "[step:  300/1150] loss: 0.070\n",
      "[step:  400/1150] loss: 0.049\n",
      "[step:  500/1150] loss: 0.073\n",
      "[step:  600/1150] loss: 0.055\n",
      "[step:  700/1150] loss: 0.059\n",
      "[step:  800/1150] loss: 0.074\n",
      "[step:  900/1150] loss: 0.061\n",
      "[step: 1000/1150] loss: 0.067\n",
      "[step: 1100/1150] loss: 0.059\n",
      "Epoch:  2, training loss: 0.062, training acc: 0.982 validation loss: 0.117, validation acc: 0.964\n",
      "[step:  100/1150] loss: 0.051\n",
      "[step:  200/1150] loss: 0.040\n",
      "[step:  300/1150] loss: 0.045\n",
      "[step:  400/1150] loss: 0.050\n",
      "[step:  500/1150] loss: 0.048\n",
      "[step:  600/1150] loss: 0.046\n",
      "[step:  700/1150] loss: 0.049\n",
      "[step:  800/1150] loss: 0.065\n",
      "[step:  900/1150] loss: 0.053\n",
      "[step: 1000/1150] loss: 0.033\n",
      "[step: 1100/1150] loss: 0.051\n",
      "Epoch:  3, training loss: 0.049, training acc: 0.987 validation loss: 0.129, validation acc: 0.965\n",
      "[step:  100/1150] loss: 0.033\n",
      "[step:  200/1150] loss: 0.033\n",
      "[step:  300/1150] loss: 0.034\n",
      "[step:  400/1150] loss: 0.027\n",
      "[step:  500/1150] loss: 0.045\n",
      "[step:  600/1150] loss: 0.037\n",
      "[step:  700/1150] loss: 0.051\n",
      "[step:  800/1150] loss: 0.036\n",
      "[step:  900/1150] loss: 0.043\n",
      "[step: 1000/1150] loss: 0.048\n",
      "[step: 1100/1150] loss: 0.048\n",
      "Epoch:  4, training loss: 0.040, training acc: 0.989 validation loss: 0.130, validation acc: 0.960\n",
      "[step:  100/1150] loss: 0.035\n",
      "[step:  200/1150] loss: 0.040\n",
      "[step:  300/1150] loss: 0.037\n",
      "[step:  400/1150] loss: 0.029\n",
      "[step:  500/1150] loss: 0.041\n",
      "[step:  600/1150] loss: 0.029\n",
      "[step:  700/1150] loss: 0.035\n",
      "[step:  800/1150] loss: 0.033\n",
      "[step:  900/1150] loss: 0.032\n",
      "[step: 1000/1150] loss: 0.040\n",
      "[step: 1100/1150] loss: 0.022\n",
      "Epoch:  5, training loss: 0.034, training acc: 0.991 validation loss: 0.137, validation acc: 0.961\n",
      "[step:  100/1150] loss: 0.016\n",
      "[step:  200/1150] loss: 0.027\n",
      "[step:  300/1150] loss: 0.039\n",
      "[step:  400/1150] loss: 0.027\n",
      "[step:  500/1150] loss: 0.031\n",
      "[step:  600/1150] loss: 0.037\n",
      "[step:  700/1150] loss: 0.037\n",
      "[step:  800/1150] loss: 0.033\n",
      "[step:  900/1150] loss: 0.026\n",
      "[step: 1000/1150] loss: 0.031\n",
      "[step: 1100/1150] loss: 0.032\n",
      "Epoch:  6, training loss: 0.030, training acc: 0.992 validation loss: 0.146, validation acc: 0.959\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "EPOCHS = 7 \n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "train_acc_history = []\n",
    "valid_acc_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(net, train_dataloader) \n",
    "    valid_loss, valid_acc = validate(net, valid_dataloader)\n",
    "    \n",
    "    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    valid_loss_history.append(valid_loss)\n",
    "\n",
    "    train_acc_history.append(train_acc)\n",
    "    valid_acc_history.append(valid_acc)\n",
    "\n",
    "    if valid_loss <= min(valid_loss_history):\n",
    "        torch.save(net.state_dict(), \"net.pt\")\n",
    "        \n",
    "# Epoch:  0, training loss: 0.400, training acc: 0.924 validation loss: 0.146, validation acc: 0.963\n",
    "# Epoch:  1, training loss: 0.086, training acc: 0.975 validation loss: 0.137, validation acc: 0.961\n",
    "# Epoch:  2, training loss: 0.062, training acc: 0.982 validation loss: 0.117, validation acc: 0.964\n",
    "# Epoch:  3, training loss: 0.049, training acc: 0.987 validation loss: 0.129, validation acc: 0.965\n",
    "# Epoch:  4, training loss: 0.040, training acc: 0.989 validation loss: 0.130, validation acc: 0.960\n",
    "# Epoch:  5, training loss: 0.034, training acc: 0.991 validation loss: 0.137, validation acc: 0.961\n",
    "# Epoch:  6, training loss: 0.030, training acc: 0.992 validation loss: 0.146, validation acc: 0.959\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524ff30",
   "metadata": {},
   "source": [
    "## predict on test dataset and submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fac536c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PretrainViT(\n",
       "  (model): VisionTransformer(\n",
       "    (conv_proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (encoder): Encoder(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): Sequential(\n",
       "        (encoder_layer_0): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_1): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_2): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_3): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_4): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_5): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_6): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_7): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_8): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_9): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_10): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_11): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_12): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_13): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_14): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_15): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_16): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_17): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_18): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_19): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_20): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_21): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_22): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_23): EncoderBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (heads): Sequential(\n",
       "      (head): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=120, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "net = PretrainViT()\n",
    "\n",
    "net.load_state_dict(torch.load(\"./net.pt\", map_location=\"cpu\"))\n",
    "\n",
    "net.to(device)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63e0667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "submit_df = pd.read_csv(\"./dog-breed-identification/sample_submission.csv\")\n",
    "test_names = submit_df[\"id\"].values\n",
    "columns = list(dataset.label_idx2name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81287c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, test_names, transform_fn):\n",
    "        self.test_names = test_names\n",
    "        self.transform = transform_fn\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.test_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.test_names[idx]\n",
    "        path = os.path.join(\"./dog-breed-identification/test\", name + \".jpg\")\n",
    "        img = Image.open(path)\n",
    "        img = self.transform(img)\n",
    "        return (img, name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f71e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = TestDataset(\n",
    "    test_names = test_names,\n",
    "    transform_fn = vit_valid_transform_fn\n",
    ")\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f6fef11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "len(columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30ef8a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "162it [27:08, 10.05s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for batch_idx, (batch_img, batch_name) in tqdm(enumerate(test_dataloader)):\n",
    "        df = pd.DataFrame(columns=[\"id\"] + columns)\n",
    "        df[\"id\"] = batch_name\n",
    "\n",
    "        batch_img = batch_img.to(device)\n",
    "        output = net(batch_img)\n",
    "        sm = F.softmax(output, dim=1)\n",
    "        df[columns] = sm.cpu().numpy()\n",
    "        dfs.append(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96232d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_submit = pd.concat(dfs)\n",
    "\n",
    "my_submit.to_csv(\"submit.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65cf54de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1493e-05, 8.4608e-06, 1.5825e-06, 2.5776e-05, 2.0882e-06, 4.5376e-06,\n",
      "         6.3245e-06, 7.9643e-06, 4.6323e-06, 1.8938e-05, 1.4869e-06, 2.2787e-05,\n",
      "         8.4207e-06, 3.7243e-06, 1.2103e-05, 1.8954e-05, 4.4983e-06, 4.2624e-06,\n",
      "         1.0047e-05, 8.3524e-06, 1.6876e-05, 1.2239e-05, 1.0477e-05, 6.9312e-06,\n",
      "         9.3976e-06, 1.0263e-05, 5.3778e-06, 3.6557e-05, 8.5943e-06, 8.0441e-06,\n",
      "         9.7913e-06, 1.1273e-05, 5.6956e-06, 2.0685e-05, 1.2340e-05, 4.5120e-06,\n",
      "         3.6703e-06, 1.4041e-05, 5.5831e-06, 1.0359e-05, 1.8812e-06, 1.4196e-06,\n",
      "         7.4255e-06, 8.2518e-05, 7.5308e-06, 6.8385e-06, 3.0926e-05, 9.0590e-06,\n",
      "         5.1992e-06, 5.4543e-06, 3.5545e-06, 3.7932e-05, 8.8038e-05, 9.8376e-01,\n",
      "         1.9835e-06, 1.3028e-02, 2.9180e-06, 1.0111e-06, 1.4640e-06, 2.1915e-06,\n",
      "         2.1954e-06, 1.6943e-05, 9.7967e-06, 3.1048e-06, 2.7619e-06, 7.7830e-06,\n",
      "         8.5651e-06, 1.7553e-05, 5.2244e-06, 5.2910e-06, 1.8366e-04, 3.6585e-06,\n",
      "         3.7353e-06, 3.9332e-06, 1.7059e-06, 8.9441e-05, 7.6373e-06, 9.6642e-06,\n",
      "         5.6950e-05, 2.6491e-06, 9.3244e-06, 4.1859e-06, 3.5855e-06, 1.2176e-05,\n",
      "         5.7666e-06, 1.2085e-05, 1.0047e-05, 6.2039e-06, 7.0016e-06, 6.2663e-06,\n",
      "         3.6231e-06, 3.2647e-06, 8.3885e-06, 1.9178e-05, 4.0546e-06, 3.4830e-06,\n",
      "         5.9009e-06, 3.1257e-06, 8.9555e-06, 1.9227e-05, 1.5235e-05, 4.1118e-06,\n",
      "         8.6547e-06, 1.7385e-03, 2.2086e-05, 2.2064e-06, 4.9049e-06, 7.0208e-06,\n",
      "         1.3339e-05, 5.0709e-06, 8.3937e-06, 2.5427e-06, 9.8451e-06, 5.3777e-06,\n",
      "         5.6110e-06, 2.9466e-06, 1.0784e-05, 5.9636e-06, 2.7843e-05, 3.3593e-06]],\n",
      "       device='mps:0')\n",
      "tensor([53], device='mps:0')\n",
      "Predicted class: border_collie\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "# load model\n",
    "net = PretrainViT()\n",
    "net.load_state_dict(torch.load(\"net.pt\", map_location=\"cpu\"))\n",
    "net.to(device)\n",
    "net.eval()\n",
    "\n",
    "image_path='my_test_bianmu.jpg'\n",
    "\n",
    "# image_path = \"path_to_your_image.jpg\"\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=channel_mean, std=channel_std)\n",
    "])\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image = image_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    output = net(image)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    print(probabilities)\n",
    "    _, predicted_label = torch.max(probabilities, dim=1)\n",
    "    print(predicted_label)\n",
    "\n",
    "\n",
    "# get the prediction result\n",
    "csv_path=\"./dog-breed-identification/labels.csv\"\n",
    "label_df = pd.read_csv(csv_path) \n",
    "label_idx2name = label_df['breed'].unique()\n",
    "\n",
    "predicted_class = label_idx2name[predicted_label.item()]\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683faad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690e4fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
